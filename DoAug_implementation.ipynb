{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "35c376f3",
   "metadata": {},
   "source": [
    "# DoAug 파라프레이즈 증강 파이프라인\n",
    "본 노트북은 **DoAug** 논문의 핵심 단계인 SFT → DPO → Selective Coreset Augmentation 을 다시 구현합니다.\n",
    "다운스트림 태스크 성능은 다루지 않고, _다양하고 의미를 유지하는 파라프레이즈 텍스트_ 생성에 집중합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "329a4960",
   "metadata": {},
   "outputs": [],
   "source": [
    "# (실행 환경에 따라 필요 모듈을 설치하세요)\n",
    "%pip install -q torch==2.5.1 transformers==4.45.2 datasets sentence-transformers peft accelerate trl==0.11.3 scikit-learn tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "783dd427",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import torch, os, json, random, ast, tensorboard\n",
    "from datasets import load_dataset, DatasetDict\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForCausalLM,\n",
    "    TrainingArguments,\n",
    "    Trainer,\n",
    "    DataCollatorForLanguageModeling,\n",
    "    DataCollatorWithPadding,\n",
    ")\n",
    "from transformers.trainer_utils import get_last_checkpoint\n",
    "from peft import LoraConfig, get_peft_model\n",
    "from trl import DPOTrainer, DPOConfig\n",
    "from sentence_transformers import SentenceTransformer, util\n",
    "from tqdm.auto import tqdm\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "BASE_MODEL = \"meta-llama/Llama-3.2-1B-Instruct\"\n",
    "WORKDIR = Path(\"./doaug_artifacts\")\n",
    "WORKDIR.mkdir(exist_ok=True)\n",
    "HF_TOKEN = os.environ[\"HF_TOKEN\"]\n",
    "SYSTEM_MESSAGE = \"You are a helpful assistant that only paraphrases.\"\n",
    "\n",
    "print(f\"Using {DEVICE}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4719691",
   "metadata": {},
   "source": [
    "1️⃣ Supervised Fine‑Tuning (SFT)\n",
    "---\n",
    "– Build 100 k (sentence, paraphrase) pairs.  \n",
    "– Train a LoRA adapter.  \n",
    "– Merge adapter into the base model.\n",
    "\n",
    "\n",
    "#### (i) Create DSFT_100k without overlap with later DDPO set\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7136326",
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_all = load_dataset(\"humarin/chatgpt-paraphrases\", split=\"train\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee8c46ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "import ast\n",
    "\n",
    "raw_dsft = raw_all.shuffle(seed=123).select(range(20_000))  # 20 k sources\n",
    "\n",
    "pairs = []\n",
    "dsft_sources = set()\n",
    "for ex in raw_dsft:\n",
    "    orig = ex[\"text\"]\n",
    "    dsft_sources.add(orig)\n",
    "    pars = ex[\"paraphrases\"]\n",
    "    if isinstance(pars, str):\n",
    "        pars = ast.literal_eval(pars)\n",
    "    for para in pars:\n",
    "        pairs.append({\"sentence\": orig, \"paraphrase\": para})\n",
    "\n",
    "assert len(pairs) == 100_000, \"DSFT size should be exactly 100 k\"\n",
    "\n",
    "dsft_path = WORKDIR / \"DSFT_100k.jsonl\"\n",
    "with open(dsft_path, \"w\", encoding=\"utf-8\") as f:\n",
    "    for p in pairs:\n",
    "        f.write(json.dumps(p, ensure_ascii=False) + \"\\n\")\n",
    "\n",
    "print(\"DSFT saved\", dsft_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a81b948",
   "metadata": {},
   "outputs": [],
   "source": [
    "pairs = []\n",
    "dsft_sources = set()\n",
    "for ex in raw_dsft:\n",
    "    orig = ex[\"text\"]\n",
    "    dsft_sources.add(orig)\n",
    "    pars = ex[\"paraphrases\"]\n",
    "    if isinstance(pars, str):\n",
    "        pars = ast.literal_eval(pars)\n",
    "    for para in pars:\n",
    "        pairs.append({\"sentence\": orig, \"paraphrase\": para})\n",
    "\n",
    "assert len(pairs) == 100_000, \"DSFT size should be exactly 100 k\"  # sanity\n",
    "\n",
    "dsft_path = WORKDIR / \"DSFT_100k.jsonl\"\n",
    "with open(dsft_path, \"w\", encoding=\"utf-8\") as f:\n",
    "    for p in pairs:\n",
    "        f.write(json.dumps(p, ensure_ascii=False) + \"\\n\")\n",
    "\n",
    "print(\"DSFT saved\", dsft_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "688bae40",
   "metadata": {},
   "source": [
    "#### (ii) Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f44247e",
   "metadata": {},
   "outputs": [],
   "source": [
    "dsft_path = WORKDIR / \"DSFT_100k.jsonl\"\n",
    "\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(BASE_MODEL, use_fast=True, token=HF_TOKEN)\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "tokenizer.padding_side = \"right\"\n",
    "\n",
    "\n",
    "def format_and_mask_chat(example):\n",
    "    chat = [\n",
    "        {\"role\": \"system\", \"content\": SYSTEM_MESSAGE},\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": f\"You will be given a sentence. Please paraphrase the sentence.\\nSentence: {example['sentence']}\",\n",
    "        },\n",
    "        {\"role\": \"assistant\", \"content\": example[\"paraphrase\"]},\n",
    "    ]\n",
    "\n",
    "    tokenized_chat = tokenizer.apply_chat_template(\n",
    "        chat, tokenize=True, add_generation_prompt=False, return_dict=True\n",
    "    )\n",
    "    input_ids = tokenized_chat[\"input_ids\"]\n",
    "    attention_mask = tokenized_chat[\"attention_mask\"]\n",
    "\n",
    "    # Assistant starts after prompt (with generation prompt)\n",
    "    prompt_ids = tokenizer.apply_chat_template(\n",
    "        chat[:-1], tokenize=True, add_generation_prompt=True\n",
    "    )\n",
    "    cut = len(prompt_ids)\n",
    "    labels = [-100] * cut + input_ids[cut:]\n",
    "\n",
    "    maxlen = tokenizer.model_max_length\n",
    "    input_ids = input_ids[:maxlen]\n",
    "    attention_mask = attention_mask[:maxlen]\n",
    "    labels = labels[:maxlen]\n",
    "    return {\"input_ids\": input_ids, \"attention_mask\": attention_mask, \"labels\": labels}\n",
    "\n",
    "\n",
    "dsft = load_dataset(\"json\", data_files=str(dsft_path))[\"train\"]\n",
    "\n",
    "tokenized_dsft = dsft.map(\n",
    "    format_and_mask_chat,\n",
    "    remove_columns=dsft.column_names,\n",
    ")\n",
    "\n",
    "print(f\"Tokenized dataset created with {len(tokenized_dsft)} examples.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0926f35e",
   "metadata": {},
   "source": [
    "#### (iii) Train LoRA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "374377c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List, Dict\n",
    "\n",
    "\n",
    "class ChatDataCollator:\n",
    "    def __init__(self, tokenizer, padding=\"longest\"):\n",
    "        self.pad = DataCollatorWithPadding(tokenizer, padding=padding)\n",
    "\n",
    "    def __call__(self, features: List[Dict]) -> Dict[str, torch.Tensor]:\n",
    "        labels = [f.pop(\"labels\") for f in features]\n",
    "\n",
    "        batch = self.pad(features)\n",
    "\n",
    "        max_len = batch[\"input_ids\"].size(1)\n",
    "        padded = [l + [-100] * (max_len - len(l)) for l in labels]\n",
    "        batch[\"labels\"] = torch.tensor(padded, dtype=torch.long)\n",
    "        return batch\n",
    "\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    BASE_MODEL,\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    device_map=\"auto\",\n",
    "    token=HF_TOKEN,\n",
    ")\n",
    "\n",
    "lora_cfg = LoraConfig(\n",
    "    r=8,\n",
    "    lora_alpha=16,\n",
    "    target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\"],\n",
    "    lora_dropout=0.05,\n",
    "    task_type=\"CAUSAL_LM\",\n",
    ")\n",
    "model = get_peft_model(model, lora_cfg)\n",
    "model.config.use_cache = False\n",
    "model.print_trainable_parameters()\n",
    "\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=str(WORKDIR / \"sft\"),\n",
    "    per_device_train_batch_size=32,\n",
    "    gradient_accumulation_steps=1,\n",
    "    learning_rate=1e-4,\n",
    "    num_train_epochs=3,\n",
    "    warmup_ratio=0.1,\n",
    "    lr_scheduler_type=\"cosine\",\n",
    "    logging_strategy=\"steps\",\n",
    "    logging_steps=100,\n",
    "    report_to=\"tensorboard\",\n",
    "    bf16=True,\n",
    "    optim=\"adamw_torch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    save_total_limit=3,\n",
    ")\n",
    "\n",
    "collator = ChatDataCollator(tokenizer, padding=\"longest\")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_dsft,\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=collator,\n",
    ")\n",
    "\n",
    "last_checkpoint = get_last_checkpoint(training_args.output_dir)\n",
    "\n",
    "print(\"Starting SFT training...\")\n",
    "trainer.train(resume_from_checkpoint=last_checkpoint)\n",
    "print(\"SFT training finished.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a236bb5",
   "metadata": {},
   "source": [
    "#### (iv) Merge LoRA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27a004d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Merging LoRA adapter and saving the final model...\")\n",
    "\n",
    "base_model = AutoModelForCausalLM.from_pretrained(\n",
    "    BASE_MODEL,\n",
    "    torch_dtype=torch.float16,\n",
    "    device_map=\"auto\",\n",
    "    token=HF_TOKEN,\n",
    ")\n",
    "\n",
    "from peft import PeftModel\n",
    "from glob import glob\n",
    "\n",
    "ckpts = sorted(\n",
    "    glob(str(training_args.output_dir) + \"/checkpoint-*\"),\n",
    "    key=lambda x: int(x.split(\"-\")[-1]),\n",
    ")\n",
    "last_checkpoint_path = ckpts[-1] if ckpts else None\n",
    "if last_checkpoint_path is None:\n",
    "    raise ValueError(\"No SFT checkpoint found to merge.\")\n",
    "\n",
    "sft_model = PeftModel.from_pretrained(base_model, last_checkpoint_path)\n",
    "\n",
    "sft_model = sft_model.merge_and_unload()\n",
    "\n",
    "sft_merged_dir = WORKDIR / \"sft_merged\"\n",
    "sft_model.save_pretrained(sft_merged_dir)\n",
    "tokenizer.save_pretrained(sft_merged_dir)\n",
    "\n",
    "print(f\"Fine-tuned model saved to: {sft_merged_dir}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8419ea7e",
   "metadata": {},
   "source": [
    "## 2️⃣ Direct Preference Optimization (DPO)\n",
    "– Build 50 k (prompt, chosen, rejected) preference triples **disjoint** from SFT.  \n",
    "– Train a new LoRA on top of the SFT‑merged model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc6b2922",
   "metadata": {},
   "source": [
    "#### (i) Build DDPO_50k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5dc5b607",
   "metadata": {},
   "outputs": [],
   "source": [
    "EMB_MODEL = \"sentence-transformers/all-MiniLM-L6-v2\"  # Instead of BERT-base CLS\n",
    "embedder = SentenceTransformer(EMB_MODEL, device=DEVICE)\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(BASE_MODEL, use_fast=True, token=HF_TOKEN)\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "tokenizer.padding_side = \"right\"\n",
    "\n",
    "raw_ddpo = [ex for ex in raw_all if ex[\"text\"] not in dsft_sources]\n",
    "raw_ddpo = random.Random(321).sample(raw_ddpo, 50_000)\n",
    "\n",
    "prefs = []\n",
    "BATCH = 64\n",
    "for i in tqdm(range(0, len(raw_ddpo), BATCH)):\n",
    "    chunk = raw_ddpo[i : i + BATCH]\n",
    "    sentences = [ex[\"text\"] for ex in chunk]\n",
    "    paraphrase_lists = [ast.literal_eval(ex[\"paraphrases\"]) for ex in chunk]\n",
    "\n",
    "    # Flatten for one-shot embed\n",
    "    flat = []\n",
    "    for src, plist in zip(sentences, paraphrase_lists):\n",
    "        flat.append(src)\n",
    "        flat.extend(plist)\n",
    "    embs = F.normalize(\n",
    "        embedder.encode(flat, convert_to_tensor=True, device=DEVICE), p=2, dim=1\n",
    "    )\n",
    "    idx = 0\n",
    "    for src, plist in zip(sentences, paraphrase_lists):\n",
    "        src_emb = embs[idx]\n",
    "        par_embs = embs[idx + 1 : idx + 1 + len(plist)]\n",
    "        idx += 1 + len(plist)\n",
    "        dists = 1 - (par_embs @ src_emb)  # cosine distance\n",
    "        iw, il = dists.argmax().item(), dists.argmin().item()\n",
    "        chosen, rejected = plist[iw], plist[il]\n",
    "\n",
    "        prompt = tokenizer.apply_chat_template(\n",
    "            [\n",
    "                {\"role\": \"system\", \"content\": SYSTEM_MESSAGE},\n",
    "                {\n",
    "                    \"role\": \"user\",\n",
    "                    \"content\": f\"Paraphrase the following sentence:\\n{src}\",\n",
    "                },\n",
    "            ],\n",
    "            tokenize=False,\n",
    "            add_generation_prompt=True,\n",
    "        )\n",
    "        for txt in (chosen, rejected):\n",
    "            if not txt.endswith(\"\\n<|im_end|>\"):\n",
    "                txt += \"\\n<|im_end|>\"\n",
    "        prefs.append({\"prompt\": prompt, \"chosen\": chosen, \"rejected\": rejected})\n",
    "\n",
    "\n",
    "ddpo_path = WORKDIR / \"DDPO_50k.jsonl\"\n",
    "with ddpo_path.open(\"w\", encoding=\"utf-8\") as f:\n",
    "    for p in prefs:\n",
    "        f.write(json.dumps(p, ensure_ascii=False) + \"\\n\")\n",
    "\n",
    "print(\"DDPO saved. Size:\", len(prefs))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d159d6e",
   "metadata": {},
   "source": [
    "#### (ii) Prepare model & LoRA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce9c8932",
   "metadata": {},
   "outputs": [],
   "source": [
    "ddpo_path = WORKDIR / \"DDPO_50k.jsonl\"\n",
    "sft_dir = WORKDIR / \"sft_merged\"\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    sft_dir,\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    device_map=\"auto\",\n",
    "    use_auth_token=HF_TOKEN,\n",
    ")\n",
    "\n",
    "lora_cfg_dpo = LoraConfig(\n",
    "    r=8,\n",
    "    lora_alpha=16,\n",
    "    target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\"],\n",
    "    lora_dropout=0.05,\n",
    "    task_type=\"CAUSAL_LM\",\n",
    ")\n",
    "model = get_peft_model(model, lora_cfg_dpo)\n",
    "model.config.use_cache = False\n",
    "\n",
    "ref_model = AutoModelForCausalLM.from_pretrained(\n",
    "    sft_dir, torch_dtype=torch.bfloat16, device_map=\"auto\", token=HF_TOKEN\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87c8b7c0",
   "metadata": {},
   "source": [
    "#### (iii) Train DPO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7617c18",
   "metadata": {},
   "outputs": [],
   "source": [
    "ddpo = load_dataset(\"json\", data_files=str(ddpo_path))[\"train\"]\n",
    "\n",
    "dpo_config = DPOConfig(\n",
    "    output_dir=str(WORKDIR / \"dpo\"),\n",
    "    max_length=256,\n",
    "    max_prompt_length=128,\n",
    "    per_device_train_batch_size=16,\n",
    "    gradient_accumulation_steps=2,\n",
    "    learning_rate=5e-6,\n",
    "    num_train_epochs=3,\n",
    "    warmup_ratio=0.1,\n",
    "    lr_scheduler_type=\"cosine\",\n",
    "    logging_steps=100,\n",
    "    bf16=True,\n",
    "    save_strategy=\"epoch\",\n",
    "    save_total_limit=3,\n",
    "    beta=0.1,\n",
    "    report_to=\"tensorboard\",\n",
    ")\n",
    "\n",
    "dpo_trainer = DPOTrainer(\n",
    "    model=model,\n",
    "    ref_model=ref_model,\n",
    "    args=dpo_config,\n",
    "    train_dataset=ddpo,\n",
    "    tokenizer=tokenizer,\n",
    ")\n",
    "\n",
    "last_checkpoint_path = get_last_checkpoint(dpo_config.output_dir)\n",
    "dpo_trainer.train(resume_from_checkpoint=last_checkpoint_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "448002d6",
   "metadata": {},
   "source": [
    "#### (iv) Merge and save"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3ab8ae1",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_model = model.merge_and_unload()\n",
    "final_dir = WORKDIR / \"doaug_paraphraser\"\n",
    "\n",
    "final_model.save_pretrained(final_dir)\n",
    "tokenizer.save_pretrained(final_dir)\n",
    "\n",
    "print(\"DPO‑finished model saved to\", final_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c62dde35",
   "metadata": {},
   "source": [
    "3️⃣ Quick Inference Check\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "57978b4d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test:\n",
      "Ambiance was restored to the space with the flicker of a flame.\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "paraphraser = pipeline(\n",
    "    \"text-generation\", model=str(final_dir), tokenizer=tokenizer, device=DEVICE\n",
    ")\n",
    "\n",
    "\n",
    "def paraphrase(sentence: str):\n",
    "    prompt = tokenizer.apply_chat_template(\n",
    "        [\n",
    "            {\"role\": \"system\", \"content\": SYSTEM_MESSAGE},\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": f\"You will be given a sentence. Please paraphrase the sentence.\\nSentence: {sentence}\",\n",
    "            },\n",
    "        ],\n",
    "        tokenize=False,\n",
    "        add_generation_prompt=True,\n",
    "    )\n",
    "    out = paraphraser(\n",
    "        prompt,\n",
    "        max_new_tokens=64,\n",
    "        do_sample=True,\n",
    "        temperature=0.7,\n",
    "        top_p=0.9,\n",
    "        eos_token_id=tokenizer.eos_token_id,\n",
    "    )[0][\"generated_text\"]\n",
    "    return (\n",
    "        out.split(\"<|start_header_id|>assistant<|end_header_id|>\\n\\n\")[-1]\n",
    "        .split(\"<|im_end|>\")[0]\n",
    "        .strip()\n",
    "    )\n",
    "\n",
    "\n",
    "print(\"Test:\")\n",
    "print(paraphrase(\"\"\"A single candle lit the dark, quiet room.\"\"\"))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "doaug",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.23"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
