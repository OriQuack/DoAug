{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf308dd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install -q torch==2.5.1 transformers==4.45.2 datasets sentence-transformers peft accelerate trl==0.11.4 scikit-learn tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27e3174c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import torch, os, json, random, ast\n",
    "from datasets import load_dataset, Dataset\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForCausalLM,\n",
    "    DataCollatorWithPadding,\n",
    "    pipeline,\n",
    ")\n",
    "from transformers.trainer_utils import get_last_checkpoint\n",
    "from peft import LoraConfig, get_peft_model, PeftModel\n",
    "from trl import DPOTrainer, DPOConfig, SFTTrainer, SFTConfig\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from tqdm.auto import tqdm\n",
    "import torch.nn.functional as F\n",
    "from typing import Dict, Optional, List\n",
    "\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "BASE_MODEL = \"meta-llama/Llama-3.2-1B-Instruct\"\n",
    "WORKDIR = Path(\"./doaug_artifacts\")\n",
    "WORKDIR.mkdir(exist_ok=True)\n",
    "HF_TOKEN = os.environ[\"HF_TOKEN\"]\n",
    "if not HF_TOKEN:\n",
    "    raise ValueError(\"Hugging Face Hub 토큰을 'HF_TOKEN' 환경 변수로 설정해주세요.\")\n",
    "\n",
    "SYSTEM_MESSAGE = \"You are a helpful assistant that only paraphrases.\"\n",
    "\n",
    "print(f\"Using {DEVICE}\")\n",
    "print(f\"Artifacts will be saved to: {WORKDIR}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36e81279",
   "metadata": {},
   "source": [
    "# 1️⃣ Supervised Fine‑Tuning (SFT)\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b51fd76b",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n--- 1. SFT Stage ---\")\n",
    "\n",
    "dsft_path = WORKDIR / \"DSFT_100k.jsonl\"\n",
    "raw_all = load_dataset(\"humarin/chatgpt-paraphrases\", split=\"train\")\n",
    "\n",
    "if dsft_path.exists():\n",
    "    print(f\"DSFT_100k.jsonl already exists at {dsft_path}. Skipping generation.\")\n",
    "else:\n",
    "    print(\"(i) Building DSFT_100k dataset...\")\n",
    "\n",
    "    rng = random.Random(321)\n",
    "    shuffled_ds = raw_all.shuffle(seed=123)\n",
    "    pairs = []\n",
    "    dsft_sources = set()\n",
    "    pbar = tqdm(total=100_000, desc=\"Generating 100k SFT pairs\")\n",
    "    for ex in shuffled_ds:\n",
    "        if len(pairs) >= 100_000:\n",
    "            break\n",
    "        orig = ex[\"text\"]\n",
    "        if orig in dsft_sources:\n",
    "            continue\n",
    "        dsft_sources.add(orig)\n",
    "        pars = ex[\"paraphrases\"]\n",
    "        if isinstance(pars, str):\n",
    "            try:\n",
    "                pars = ast.literal_eval(pars)\n",
    "            except (ValueError, SyntaxError):\n",
    "                continue\n",
    "        for para in pars:\n",
    "            if len(pairs) >= 100_000:\n",
    "                break\n",
    "            pairs.append({\"sentence\": orig, \"paraphrase\": para})\n",
    "            pbar.update(1)\n",
    "    pbar.close()\n",
    "    assert len(pairs) == 100_000\n",
    "\n",
    "    with open(dsft_path, \"w\", encoding=\"utf-8\") as f:\n",
    "        for p in pairs:\n",
    "            f.write(json.dumps(p, ensure_ascii=False) + \"\\n\")\n",
    "    print(f\"DSFT saved to {dsft_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "184dd218",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"(ii) Tokenizing dataset for SFT...\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(BASE_MODEL, use_fast=True, token=HF_TOKEN)\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "tokenizer.padding_side = \"right\"\n",
    "\n",
    "\n",
    "def format_and_mask_chat(example: Dict) -> Optional[Dict]:\n",
    "    # ... (이전과 동일한 안정적인 마스킹 로직) ...\n",
    "    chat_with_assistant = [\n",
    "        {\"role\": \"system\", \"content\": SYSTEM_MESSAGE},\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": f\"You will be given a sentence. Please paraphrase the sentence.\\nSentence: {example['sentence']}\",\n",
    "        },\n",
    "        {\"role\": \"assistant\", \"content\": example[\"paraphrase\"]},\n",
    "    ]\n",
    "    chat_prompt_only = chat_with_assistant[:-1]\n",
    "    prompt_ids = tokenizer.apply_chat_template(\n",
    "        chat_prompt_only, tokenize=True, add_generation_prompt=True\n",
    "    )\n",
    "    full_ids = tokenizer.apply_chat_template(\n",
    "        chat_with_assistant, tokenize=True, add_generation_prompt=False\n",
    "    )\n",
    "    maxlen = tokenizer.model_max_length\n",
    "    if len(prompt_ids) >= maxlen:\n",
    "        return None\n",
    "    labels = full_ids.copy()\n",
    "    labels[: len(prompt_ids)] = [-100] * len(prompt_ids)\n",
    "    input_ids = full_ids[:maxlen]\n",
    "    attention_mask = [1] * len(input_ids)\n",
    "    labels = labels[:maxlen]\n",
    "    if all(l == -100 for l in labels):\n",
    "        return None\n",
    "    if len(labels) < len(input_ids):\n",
    "        labels += [-100] * (len(input_ids) - len(labels))\n",
    "    return {\"input_ids\": input_ids, \"attention_mask\": attention_mask, \"labels\": labels}\n",
    "\n",
    "\n",
    "dsft = load_dataset(\"json\", data_files=str(dsft_path))[\"train\"]\n",
    "tokenized_dsft = dsft.map(format_and_mask_chat, remove_columns=dsft.column_names)\n",
    "print(\n",
    "    f\"Tokenized SFT dataset created. {len(dsft) - len(tokenized_dsft)} examples were filtered out.\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89d03853",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"(iii) Preparing model and training SFT LoRA adapter...\")\n",
    "\n",
    "\n",
    "class ChatDataCollator:\n",
    "    def __init__(self, tokenizer, padding=\"longest\"):\n",
    "        self.pad = DataCollatorWithPadding(tokenizer, padding=padding)\n",
    "\n",
    "    def __call__(self, features: List[Dict]) -> Dict[str, torch.Tensor]:\n",
    "        labels = [f.pop(\"labels\") for f in features]\n",
    "\n",
    "        batch = self.pad(features)\n",
    "\n",
    "        max_len = batch[\"input_ids\"].size(1)\n",
    "        padded = [l + [-100] * (max_len - len(l)) for l in labels]\n",
    "        batch[\"labels\"] = torch.tensor(padded, dtype=torch.long)\n",
    "        return batch\n",
    "\n",
    "\n",
    "collator = ChatDataCollator(tokenizer, padding=\"longest\")\n",
    "\n",
    "# 논문 명세: Llama-3.2-1B-Instruct with BF16 (부록 C)\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    BASE_MODEL, torch_dtype=torch.bfloat16, device_map=\"auto\", token=HF_TOKEN\n",
    ")\n",
    "# 논문 명세: LoRA rank r = 8 (부록 C)\n",
    "lora_cfg = LoraConfig(\n",
    "    r=8,\n",
    "    lora_alpha=16,\n",
    "    target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\"],\n",
    "    lora_dropout=0.05,\n",
    "    task_type=\"CAUSAL_LM\",\n",
    ")\n",
    "model = get_peft_model(model, lora_cfg)\n",
    "model.config.use_cache = False\n",
    "model.print_trainable_parameters()\n",
    "\n",
    "# 논문 부록 C의 SFT 단계 하이퍼파라미터 설정\n",
    "sft_cfg = SFTConfig(\n",
    "    output_dir=str(WORKDIR / \"sft\"),\n",
    "    per_device_train_batch_size=16,\n",
    "    gradient_accumulation_steps=2,\n",
    "    learning_rate=1e-4,\n",
    "    num_train_epochs=3,\n",
    "    warmup_ratio=0.1,\n",
    "    lr_scheduler_type=\"cosine\",\n",
    "    logging_steps=100,\n",
    "    bf16=True,\n",
    "    optim=\"adamw_torch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    save_total_limit=3,\n",
    "    max_seq_length=tokenizer.model_max_length,\n",
    ")\n",
    "\n",
    "trainer = SFTTrainer(\n",
    "    model,\n",
    "    train_dataset=tokenized_dsft,\n",
    "    args=sft_cfg,\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=collator,\n",
    ")\n",
    "trainer.train(resume_from_checkpoint=get_last_checkpoint(sft_cfg.output_dir))\n",
    "print(\"SFT training finished.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7239ecd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"(iv) Merging SFT LoRA adapter and saving the final model...\")\n",
    "del trainer, model\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "base_model = AutoModelForCausalLM.from_pretrained(\n",
    "    BASE_MODEL, torch_dtype=torch.bfloat16, device_map=\"auto\", token=HF_TOKEN\n",
    ")\n",
    "ckpts = sorted(\n",
    "    Path(sft_cfg.output_dir).glob(\"checkpoint-*\"),\n",
    "    key=lambda x: int(x.name.split(\"-\")[-1]),\n",
    ")\n",
    "if not ckpts:\n",
    "    raise ValueError(\"No SFT checkpoint found.\")\n",
    "last_checkpoint_path = ckpts[-1]\n",
    "\n",
    "sft_model = PeftModel.from_pretrained(base_model, str(last_checkpoint_path))\n",
    "sft_model = sft_model.merge_and_unload()\n",
    "\n",
    "sft_merged_dir = WORKDIR / \"sft_merged\"\n",
    "sft_model.save_pretrained(sft_merged_dir)\n",
    "tokenizer.save_pretrained(sft_merged_dir)\n",
    "print(f\"SFT-merged model saved to: {sft_merged_dir}\")\n",
    "del base_model, sft_model\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50bff6fe",
   "metadata": {},
   "source": [
    "# 2️⃣ Direct Preference Optimization (DPO)\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea3dcc4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n--- 2. DPO Stage ---\")\n",
    "ddpo_path = WORKDIR / \"DDPO_50k.jsonl\"\n",
    "\n",
    "if ddpo_path.exists():\n",
    "    print(f\"DDPO_50k already exists at {ddpo_path}. Skipping generation.\")\n",
    "else:\n",
    "    print(\"(i) Building DDPO_50k dataset...\")\n",
    "    EMB_MODEL = \"sentence-transformers/all-MPNet-base-v2\"\n",
    "    embedder = SentenceTransformer(EMB_MODEL, device=DEVICE)\n",
    "    raw_ddpo_candidates = [ex for ex in raw_all if ex[\"text\"] not in dsft_sources]\n",
    "    raw_ddpo = rng.sample(raw_ddpo_candidates, 50_000)\n",
    "\n",
    "    prefs = []\n",
    "    BATCH = 64\n",
    "    for i in tqdm(range(0, len(raw_ddpo), BATCH), desc=\"Building DPO dataset\"):\n",
    "        chunk = raw_ddpo[i : i + BATCH]\n",
    "        sentences = [ex[\"text\"] for ex in chunk]\n",
    "        paraphrase_lists = [\n",
    "            (\n",
    "                ast.literal_eval(ex[\"paraphrases\"])\n",
    "                if isinstance(ex[\"paraphrases\"], str)\n",
    "                else ex[\"paraphrases\"]\n",
    "            )\n",
    "            for ex in chunk\n",
    "        ]\n",
    "        flat, valid_indices = [], []\n",
    "        for j, (src, plist) in enumerate(zip(sentences, paraphrase_lists)):\n",
    "            if isinstance(plist, list) and len(plist) >= 2:\n",
    "                flat.append(src)\n",
    "                flat.extend(plist)\n",
    "                valid_indices.append(j)\n",
    "        if not flat:\n",
    "            continue\n",
    "        embs = F.normalize(\n",
    "            embedder.encode(flat, convert_to_tensor=True, device=DEVICE), p=2, dim=1\n",
    "        )\n",
    "        idx = 0\n",
    "        for j in valid_indices:\n",
    "            src, plist = sentences[j], paraphrase_lists[j]\n",
    "            src_emb = embs[idx]\n",
    "            par_embs = embs[idx + 1 : idx + 1 + len(plist)]\n",
    "            idx += 1 + len(plist)\n",
    "            dists = 1 - (par_embs @ src_emb)\n",
    "            if dists.numel() < 2:\n",
    "                continue\n",
    "            iw, il = dists.argmax().item(), dists.argmin().item()\n",
    "            if iw == il:\n",
    "                continue\n",
    "            chosen, rejected = plist[iw], plist[il]\n",
    "            prompt_chat = [\n",
    "                {\"role\": \"system\", \"content\": SYSTEM_MESSAGE},\n",
    "                {\n",
    "                    \"role\": \"user\",\n",
    "                    \"content\": f\"You will be given a sentence. Please paraphrase the sentence.\\nSentence: {src}\",\n",
    "                },\n",
    "            ]\n",
    "            prompt_str = tokenizer.apply_chat_template(\n",
    "                prompt_chat, tokenize=False, add_generation_prompt=True\n",
    "            )\n",
    "            prefs.append({\"prompt\": prompt_str, \"chosen\": chosen, \"rejected\": rejected})\n",
    "\n",
    "    with ddpo_path.open(\"w\", encoding=\"utf-8\") as f:\n",
    "        for p in prefs:\n",
    "            f.write(json.dumps(p, ensure_ascii=False) + \"\\n\")\n",
    "    print(f\"DPO dataset saved. Size: {len(prefs)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9f85068",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"(ii) Preparing model for DPO training...\")\n",
    "sft_dir = WORKDIR / \"sft_merged\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    sft_dir,\n",
    "    use_fast=True,\n",
    "    token=HF_TOKEN,\n",
    ")\n",
    "\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "tokenizer.padding_side = \"right\"\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    sft_dir, torch_dtype=torch.bfloat16, device_map=\"auto\", token=HF_TOKEN\n",
    ")\n",
    "lora_cfg_dpo = LoraConfig(\n",
    "    r=8,\n",
    "    lora_alpha=16,\n",
    "    target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\"],\n",
    "    lora_dropout=0.05,\n",
    "    task_type=\"CAUSAL_LM\",\n",
    ")\n",
    "model = get_peft_model(model, lora_cfg_dpo)\n",
    "ref_model = AutoModelForCausalLM.from_pretrained(\n",
    "    sft_dir, torch_dtype=torch.bfloat16, device_map=\"auto\", token=HF_TOKEN\n",
    ")\n",
    "ref_model.requires_grad_(False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c536c958",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"(iii) Starting DPO training...\")\n",
    "ddpo_dataset = load_dataset(\"json\", data_files=str(ddpo_path))[\"train\"]\n",
    "\n",
    "dpo_config = DPOConfig(\n",
    "    output_dir=str(WORKDIR / \"dpo\"),\n",
    "    max_length=256,\n",
    "    max_prompt_length=128,\n",
    "    per_device_train_batch_size=16,\n",
    "    gradient_accumulation_steps=2,\n",
    "    learning_rate=5e-6,\n",
    "    num_train_epochs=3,\n",
    "    warmup_ratio=0.1,\n",
    "    lr_scheduler_type=\"cosine\",\n",
    "    logging_steps=100,\n",
    "    bf16=True,\n",
    "    save_strategy=\"epoch\",\n",
    "    save_total_limit=3,\n",
    "    beta=0.1,\n",
    "    report_to=\"tensorboard\",\n",
    ")\n",
    "\n",
    "dpo_trainer = DPOTrainer(\n",
    "    model=model,\n",
    "    ref_model=ref_model,\n",
    "    args=dpo_config,\n",
    "    train_dataset=ddpo_dataset,\n",
    "    tokenizer=tokenizer,\n",
    ")\n",
    "dpo_trainer.train(resume_from_checkpoint=get_last_checkpoint(dpo_config.output_dir))\n",
    "print(\"DPO training finished.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "769eb1b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"(iv) Merging DPO adapter and saving the final model...\")\n",
    "del dpo_trainer, model\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "final_model_peft = PeftModel.from_pretrained(\n",
    "    AutoModelForCausalLM.from_pretrained(\n",
    "        sft_dir, torch_dtype=torch.bfloat16, device_map=\"auto\", token=HF_TOKEN\n",
    "    ),\n",
    "    get_last_checkpoint(dpo_config.output_dir),\n",
    ")\n",
    "final_model = final_model_peft.merge_and_unload()\n",
    "final_dir = WORKDIR / \"doaug_paraphraser\"\n",
    "final_model.save_pretrained(final_dir)\n",
    "tokenizer.save_pretrained(final_dir)\n",
    "print(f\"DPO-finished model saved to {final_dir}\")\n",
    "del final_model, final_model_peft\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c007df01",
   "metadata": {},
   "source": [
    "# 3️⃣ Quick Inference Check\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6edfbc49",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n--- 3. Inference Check ---\")\n",
    "paraphraser = pipeline(\n",
    "    \"text-generation\",\n",
    "    model=str(final_dir),\n",
    "    tokenizer=tokenizer,\n",
    "    device=0,\n",
    "    torch_dtype=torch.bfloat16,\n",
    ")\n",
    "\n",
    "\n",
    "def paraphrase(sentence: str):\n",
    "    prompt_chat = [\n",
    "        {\"role\": \"system\", \"content\": SYSTEM_MESSAGE},\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": f\"You will be given a sentence. Please paraphrase the sentence.\\nSentence: {sentence}\",\n",
    "        },\n",
    "    ]\n",
    "    prompt = tokenizer.apply_chat_template(\n",
    "        prompt_chat, tokenize=False, add_generation_prompt=True\n",
    "    )\n",
    "    generated_text = paraphraser(\n",
    "        prompt,\n",
    "        max_new_tokens=64,\n",
    "        do_sample=True,\n",
    "        temperature=0.7,\n",
    "        top_p=0.9,\n",
    "        eos_token_id=tokenizer.eos_token_id,\n",
    "    )[0][\"generated_text\"]\n",
    "    response_part = (\n",
    "        generated_text.split(prompt, 1)[-1].split(tokenizer.eos_token, 1)[0].strip()\n",
    "    )\n",
    "    return response_part"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82040a43",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nTest Paraphrasing:\")\n",
    "test_sentence = \"A single candle lit the dark, quiet room.\"\n",
    "print(f\"Original: {test_sentence}\")\n",
    "paraphrased_text = paraphrase(test_sentence)\n",
    "print(f\"Paraphrased: {paraphrased_text}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "doaug",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.23"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
